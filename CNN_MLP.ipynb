{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 103011110 吳幸儒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    '''Multiple Layer Perceptron'''\n",
    "    \n",
    "    def __init__(self, Layers=(2, 5, 3), BatchSize = 4):\n",
    "        self.bs = BatchSize\n",
    "        self.net = [dict() for i in range(len(Layers))] # A list of fully-connected Layers\n",
    "       \n",
    "        self.net[0]['i'] = np.zeros((Layers[0], self.bs), dtype='float64')\n",
    "        self.net[0]['z'] = np.zeros((Layers[0], self.bs), dtype='float64')\n",
    "        self.net[0]['bnz'] = np.zeros((Layers[0], self.bs), dtype='float64')\n",
    "        self.net[0]['a'] = np.zeros((Layers[0], self.bs), dtype='float64')    \n",
    "            \n",
    "        self.net[0]['dJdi'] = np.zeros(self.net[0]['i'].shape, dtype='float64')\n",
    "        self.net[0]['dJdz'] = np.zeros(self.net[0]['z'].shape, dtype='float64') \n",
    "        self.net[0]['dJdbnz'] = np.zeros(self.net[0]['bnz'].shape, dtype='float64')\n",
    "        self.net[0]['dJda'] = np.zeros(self.net[0]['a'].shape, dtype='float64')\n",
    "        \n",
    "        for i in range(1, len(Layers), 1):\n",
    "            self.net[i]['i'] = np.zeros((Layers[i-1], self.bs), dtype='float64')\n",
    "            self.net[i]['z'] = np.zeros((Layers[i], self.bs), dtype='float64')\n",
    "            self.net[i]['bnz'] = np.zeros((Layers[i], self.bs), dtype='float64')\n",
    "            self.net[i]['a'] = np.zeros((Layers[i], self.bs), dtype='float64')\n",
    "            self.net[i]['W'] = np.random.randn(Layers[i], Layers[i-1]).astype('float64')\n",
    "            self.net[i]['b'] = np.random.randn(Layers[i], 1).astype('float64')\n",
    "            self.net[i]['gamma'] = np.random.randn(Layers[i], 1).astype('float64')\n",
    "            self.net[i]['beta'] = np.random.randn(Layers[i], 1).astype('float64')\n",
    "            \n",
    "            self.net[i]['dJdi'] = np.zeros(self.net[i]['i'].shape, dtype='float64')\n",
    "            self.net[i]['dJdz'] = np.zeros(self.net[i]['z'].shape, dtype='float64')\n",
    "            self.net[i]['dJdbnz'] = np.zeros(self.net[i]['bnz'].shape, dtype='float64')\n",
    "            self.net[i]['dJda'] = np.zeros(self.net[i]['a'].shape, dtype='float64')\n",
    "            self.net[i]['dJdW'] = np.zeros(self.net[i]['W'].shape, dtype='float64')\n",
    "            self.net[i]['dJdb'] = np.zeros(self.net[i]['b'].shape, dtype='float64')\n",
    "            self.net[i]['dJdgamma'] = np.zeros(self.net[i]['gamma'].shape, dtype='float64')\n",
    "            self.net[i]['dJdbeta'] = np.zeros(self.net[i]['beta'].shape, dtype='float64')\n",
    "        \n",
    "        self.p = np.zeros(self.net[-1]['a'].shape, dtype='float64') # Softmax Output\n",
    "        self.dJdp = np.zeros(self.p.shape, dtype='float64')\n",
    "        \n",
    "        self.yhat = np.zeros(self.bs, dtype=int) # Predicted Answer\n",
    "        self.y_onehot = np.zeros(self.p.shape, dtype=int) # One-Hot-Encoded Predicted Answer\n",
    "        \n",
    "        self.J = [] # Loss value trace\n",
    "        self.J_val = [] # Loss value trace for validation\n",
    "        self.L2_regularization = [] # L2 regularization value trace\n",
    "        \n",
    "    \n",
    "    # Feed Forward Evaluation\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # For the input layer, Just copy input to Layer 0\n",
    "        np.copyto(self.net[0]['i'], x)\n",
    "        np.copyto(self.net[0]['z'], self.net[0]['i'])\n",
    "        np.copyto(self.net[0]['bnz'], self.net[0]['z'])\n",
    "        np.copyto(self.net[0]['a'], self.net[0]['bnz'])\n",
    "        \n",
    "        for i in range(1, len(self.net)): # Frome Layer 1 to the Last layer\n",
    "             \n",
    "            np.copyto(self.net[i]['i'], self.net[i-1]['a']) # Copy last layer's output \n",
    "            np.copyto(self.net[i]['z'],\n",
    "                      np.dot(self.net[i]['W'], self.net[i]['i']) + self.net[i]['b'])           \n",
    "            np.copyto(self.net[i]['bnz'], \n",
    "                      self.batch_norm(self.net[i]['z'],\n",
    "                                      self.net[i]['gamma'],\n",
    "                                      self.net[i]['beta'],\n",
    "                                      0.01))  # 0.01 is epsilon\n",
    "            np.copyto(self.net[i]['a'], \n",
    "                      self.ReLU(self.net[i]['bnz']))# Activation function\n",
    "            \n",
    "            \n",
    "        np.copyto(self.p, self.softmax(self.net[-1]['a']))  # Softmax\n",
    "        np.copyto(self.yhat, np.argmax(self.p, axis=0)) # Prediction result\n",
    "        \n",
    "        return\n",
    "   \n",
    "    # Batch Normalization for an MLP Layer\n",
    "    # x --> bnx\n",
    "    def batch_norm(self, x, gamma, beta, eps):\n",
    "        \n",
    "        bs = x.shape[-1]\n",
    "        mu = 1.0/bs * np.sum(x, axis=-1)[:, None]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=-1)[:, None]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        bnx = gamma * xhat + beta\n",
    "        \n",
    "        return bnx\n",
    "    \n",
    "    # Backprop Batch Normalization for an MLP Layer\n",
    "    # (dJdbnx, x, gamma, beta) --> (dJdbeta, dJdgamma, dJdx)\n",
    "    def batch_norm_prime(self, dJdbnx, x, gamma, beta, eps):\n",
    "        \n",
    "        bs = x.shape[-1]\n",
    "        mu = 1.0/bs * np.sum(x, axis=-1)[:, None]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=-1)[:, None]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        \n",
    "        dJdbeta = np.mean(dJdbnx, axis=-1)[:, None]\n",
    "        dJdgamma = np.mean(dJdbnx * xhat, axis=-1)[:, None]\n",
    "        dJdx = (1.0 - 1.0/bs) * (1.0/v - u**2 / v**3 / bs) * gamma * dJdbnx\n",
    "        \n",
    "        return dJdbeta, dJdgamma, dJdx\n",
    "    \n",
    "    def softmax(self, a):\n",
    "        return np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    \n",
    "    # Sigmoid activation function: z --> a\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-1.0*z))\n",
    "    \n",
    "    # a --> dadz\n",
    "    def sigmoidPrime(self, a):\n",
    "        return a * (1.0 - a)\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    def tanhPrime(self, a):\n",
    "        return (1.0 - a ** 2) # Derivative of tanh is (1.0 - tanh ** 2)\n",
    "    \n",
    "    def swish(self, z):\n",
    "        return z * self.sigmoid(z)\n",
    "    \n",
    "    def swishPrime(self, a, z):\n",
    "        return a + self.sigmoid(z) * (1.0 - a) \n",
    "    \n",
    "    def ReLU(self, z):\n",
    "        a = np.copy(z)\n",
    "        a[a<0] = 0.0\n",
    "        return a\n",
    "    \n",
    "    def ReLUPrime(self, a):\n",
    "        dadz = np.copy(a)\n",
    "        dadz[a>0] = 1.0\n",
    "        return dadz\n",
    "    \n",
    "    def LeakyReLU(self, z, leakyrate):\n",
    "        a = np.copy(z)\n",
    "        (rows, cols) = a.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                a[i, j] = z[i, j] if z[i,j] > 0 else (leakyrate * z[i, j])\n",
    "        return a\n",
    "    \n",
    "    def LeakyReLUPrime(self, a, leakyrate):\n",
    "        dadz = np.copy(a)\n",
    "        (rows, cols) = dadz.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                dadz[i, j] = 1.0 if a[i,j] > 0 else leakyrate\n",
    "        return dadz\n",
    "    \n",
    "    # Loss function\n",
    "    def loss(self, y, eta):\n",
    "        \n",
    "        self.y_onehot.fill(0)\n",
    "        for i in range(self.bs):\n",
    "            self.y_onehot[y[i], i] = 1\n",
    "        self.J.append(-1.0 * np.sum(self.y_onehot * np.log(self.p) / self.bs))\n",
    "        \n",
    "        L2 = 0.0 # L2 Regularization\n",
    "        for i in range(1, len(self.net)):\n",
    "            L2 += np.sum(self.net[i]['W'] ** 2)              \n",
    "        self.L2_regularization.append(eta / 2 * L2) # Only the MLP part\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Loss function for validation\n",
    "    def loss_val(self, y, eta):\n",
    "        \n",
    "        self.y_onehot.fill(0)\n",
    "        for i in range(self.bs):\n",
    "            self.y_onehot[y[i], i] = 1     \n",
    "        self.J_val.append(-1.0 * np.sum(self.y_onehot * np.log(self.p) / self.bs))\n",
    "        \n",
    "        return\n",
    "            \n",
    "    # Backward Propagation\n",
    "    def backprop(self):\n",
    "        \n",
    "        self.dJdp = 1.0 / (1.0 - self.y_onehot - self.p)\n",
    "        dpda = np.array([[self.p[i, :] * (1.0-self.p[j, :]) if i == j\n",
    "                          else -1 * self.p[i, :] * self.p[j, :]\n",
    "                          for i in range(self.p.shape[0])]\n",
    "                         for j in range(self.p.shape[0])])\n",
    "        for i in range(self.bs):\n",
    "            self.net[-1]['dJda'][:, i] = np.dot(dpda[:, :, i], self.dJdp[:, i])\n",
    "            \n",
    "        for i in range(len(self.net)-1, 0, -1):\n",
    "            \n",
    "            np.copyto(self.net[i]['dJdbnz'],\n",
    "                      (self.net[i]['dJda'] * self.ReLUPrime(self.net[i]['a'])))       \n",
    "           \n",
    "            dJdbeta, dJdgamma, dJdBNinput = self.batch_norm_prime(self.net[i]['dJdbnz'],\n",
    "                                                                  self.net[i]['z'],\n",
    "                                                                  self.net[i]['gamma'],\n",
    "                                                                  self.net[i]['beta'],\n",
    "                                                                  0.01)  \n",
    "            np.copyto(self.net[i]['dJdbeta'], dJdbeta)\n",
    "            np.copyto(self.net[i]['dJdgamma'], dJdgamma)\n",
    "            np.copyto(self.net[i]['dJdz'], dJdBNinput) \n",
    "            \n",
    "            np.copyto(self.net[i]['dJdb'],\n",
    "                      np.mean(self.net[i]['dJdz'], axis = -1)[:, None])         \n",
    "            np.copyto(self.net[i]['dJdW'],\n",
    "                      np.dot(self.net[i]['dJdz'], self.net[i]['i'].T) / self.bs)  \n",
    "            np.copyto(self.net[i]['dJdi'],\n",
    "                      np.dot(self.net[i]['W'].T, self.net[i]['dJdz']))\n",
    "            \n",
    "            # Copy gradient at the input to be the output gradient of the previous layer\n",
    "            np.copyto(self.net[i-1]['dJda'], self.net[i]['dJdi'])\n",
    "        \n",
    "        # Layer 0 does nothing but passing gradients backward\n",
    "        np.copyto(self.net[0]['dJdbnz'], self.net[0]['dJda'])\n",
    "        np.copyto(self.net[0]['dJdz'], self.net[0]['dJdbnz'])\n",
    "        np.copyto(self.net[0]['dJdi'], self.net[0]['dJdz']) \n",
    "        return\n",
    "    \n",
    "    # Update parameters\n",
    "    def update(self, lr, eta):\n",
    "        \n",
    "        # Update W, b, gamma, beta with Weight Decay from Layer 1 to the last\n",
    "        # Layer 0 has no parameters\n",
    "        for i in range(1, len(self.net)):\n",
    "            np.copyto(self.net[i]['W'],\n",
    "                      (1.0 - eta * lr) * self.net[i]['W'] - lr*self.net[i]['dJdW'])\n",
    "            np.copyto(self.net[i]['b'],\n",
    "                      (1.0 - eta * lr) * self.net[i]['b'] - lr*self.net[i]['dJdb'])\n",
    "            np.copyto(self.net[i]['gamma'],\n",
    "                      (1.0 - eta * lr) * self.net[i]['gamma'] - lr*self.net[i]['dJdgamma'])\n",
    "            np.copyto(self.net[i]['beta'],\n",
    "                      (1.0 - eta * lr) * self.net[i]['beta'] - lr*self.net[i]['dJdbeta'])\n",
    "        return\n",
    "    \n",
    "    # Train MLP alone. For a CNN, training is via the CNN instance\n",
    "    def train(self, train_x, train_y, epoch_count, lr, eta):\n",
    "        \n",
    "        for e in range(epoch_count):\n",
    "            # print (\"Epoch \", e)\n",
    "            for i in range(train_x.shape[1]//self.bs):\n",
    "                x = train_x[:, i*self.bs:(i+1)*self.bs]\n",
    "                y = train_y[i*self.bs:(i+1)*self.bs]\n",
    "                self.forward(x)\n",
    "                self.loss(y, eta)\n",
    "                self.backprop()\n",
    "                self.update(lr, eta)           \n",
    "        return  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Layer(object):\n",
    "    '''Convolution Layer including 2D convolution, pooling, activation and batch normalization'''\n",
    "    def __init__(self,\n",
    "                 bs,    #batch size\n",
    "                 i_ch,  #input channel\n",
    "                 i_h,  #input height\n",
    "                 i_w,  #input weight\n",
    "                 k_num, # k_num == o_ch; number of kernels equal number of output channels\n",
    "                 k_h,   #kernel height\n",
    "                 k_w,   #kernel weight\n",
    "                 stride = 1, # Stride\n",
    "                 zp = 0, # Zero Padding\n",
    "                 mph = 2, # Pooling kernel height\n",
    "                 mpw = 2, # Pooling kernel width\n",
    "                 leakyrate = 0.1):\n",
    "        \n",
    "        #convert to layer\n",
    "        self.bs = bs\n",
    "        self.i_ch = i_ch\n",
    "        self.i_h = i_h\n",
    "        self.i_w = i_w\n",
    "        self.k_num = k_num\n",
    "        self.k_h = k_h\n",
    "        self.k_w = k_w\n",
    "        self.stride = stride\n",
    "        self.zp = zp\n",
    "        self.mph = mph\n",
    "        self.mpw = mpw\n",
    "        self.leakyrate = leakyrate\n",
    "       \n",
    "        # conv2d(ai, W, b) --> z\n",
    "        # activation(z) --> zr\n",
    "        # Batch_norm(zr, gamma, beta) --> r\n",
    "        # Pool(r) --> ao, mp_trace\n",
    "        self.ai = np.zeros((bs, i_ch, i_h+2*zp, i_w+2*zp), dtype='float64')\n",
    "        self.z = np.zeros((bs, k_num, (i_h - k_h+1)//stride, (i_w - k_w+1)//stride), dtype='float64')\n",
    "        self.bnz = np.zeros(self.z.shape, dtype='float64')\n",
    "        self.r = np.zeros(self.z.shape, dtype='float64')\n",
    "        self.mp_trace = np.zeros(self.r.shape, dtype='int32')\n",
    "        self.ao = np.zeros((bs, k_num, (i_h - k_h+1)//stride//mph, (i_w - k_w+1)//stride//mpw), dtype='float64')\n",
    "        self.W = np.random.randn(k_num, i_ch, k_h, k_w).astype('float64')\n",
    "        self.b = np.random.randn(k_num, 1).astype('float64')\n",
    "        self.gamma = np.ones(self.z.shape[1:], dtype='float64')\n",
    "        self.beta = np.zeros(self.z.shape[1:], dtype='float64')\n",
    "        \n",
    "        self.dJdai = np.zeros(self.ai.shape, dtype='float64')\n",
    "        self.dJdz = np.zeros(self.z.shape, dtype='float64')\n",
    "        self.dJdbnz = np.zeros(self.bnz.shape, dtype='float64')\n",
    "        self.dJdr = np.zeros(self.r.shape, dtype='float64')\n",
    "        self.dJdao = np.zeros(self.ao.shape, dtype='float64')\n",
    "        self.dJdW = np.zeros(self.W.shape, dtype='float64')\n",
    "        self.dJdb = np.zeros(self.b.shape, dtype='float64')\n",
    "        self.dJdgamma = np.zeros(self.gamma.shape, dtype='float64')\n",
    "        self.dJdbeta = np.zeros(self.beta.shape, dtype='float64')\n",
    "        \n",
    "    def conv2d(self):  # (ai, W, b) --> z\n",
    "        \n",
    "        (i_bs, i_ch, i_rows, i_cols) = self.ai.shape # Input data\n",
    "        (k_num, k_ch, k_rows, k_cols) = self.W.shape # Kernels\n",
    "        (z_bs, z_ch, z_rows, z_cols) = self.z.shape   # Output z_ch == k_num\n",
    "        # self.ai.fill(0.0)\n",
    "        # np.copyto(self.ai[:, :, self.zp:self.zp+i_rows, self.zp:self.zp+i_cols], i_data)\n",
    "        s = self.stride\n",
    "        \n",
    "        for b in range(z_bs): # Step over mini batch\n",
    "            for c in range(z_ch): # z_ch == k_num\n",
    "                for i in range(z_rows):\n",
    "                    for j in range(z_cols): \n",
    "                        self.z[b, c, i, j] = np.sum(np.multiply(self.W[c, :,  :, :],   #multipy two matrix\n",
    "                                                                self.ai[b,\n",
    "                                                                        :,\n",
    "                                                                        i*s : i*s+k_rows,\n",
    "                                                                        j*s : j*s+k_cols])\n",
    "                                                   ) + self.b[c]\n",
    "        return \n",
    "        \n",
    "    \n",
    "    def conv2d_prime(self): # (dJdz, W, ai) --> (dJdai, dJdW, dJdb)\n",
    "        (z_bs, z_ch, z_rows, z_cols) = self.dJdz.shape\n",
    "        (a_bs, a_ch, a_rows, a_cols) = self.dJdai.shape\n",
    "        (k_num, k_ch, k_rows, k_cols) = self.dJdW.shape\n",
    "        (b_num, b_ch) = self.dJdb.shape\n",
    "        s = self.stride # self.stride\n",
    "        self.dJdai.fill(0.0)\n",
    "        self.dJdW.fill(0.0)\n",
    "        self.dJdb.fill(0.0)\n",
    "        \n",
    "        # For each single dJdZ value, add a kernels to dJdai\n",
    "        for b in range(z_bs):\n",
    "            for c in range(z_ch):\n",
    "                for i in range(z_rows):\n",
    "                    for j in range(z_cols):\n",
    "                        dJdz_value = self.dJdz[b, c, i, j]\n",
    "                        self.dJdai[b, :, i*s:i*s+k_rows, j*s:j*s+k_cols] += dJdz_value * self.W[c, :, :, :]\n",
    "        \n",
    "        # Each dJdW value is a dot product of two arrays              \n",
    "        for k in range(k_num):\n",
    "            for c in range(k_ch):\n",
    "                for i in range(k_rows):\n",
    "                    for j in range(k_cols):\n",
    "                        self.dJdW[k, c, i, j] = np.sum(self.ai[:, c, i:i+z_rows*s:s, j:j+z_cols*s:s] *\n",
    "                                                       self.dJdz[:, k, :, :]) / self.bs \n",
    "        \n",
    "        # Each dJdb value is a sum of a whole dJdZ array\n",
    "        for c in range(z_ch):\n",
    "            self.dJdb[c] = np.sum(self.dJdz[:, c, :, :]) / self.bs\n",
    "            \n",
    "        return \n",
    "\n",
    "    # All activation functions produce  z --> zr  \n",
    "    def LeakyReLU(self): \n",
    "        np.copyto(self.r, np.where(self.z > 0, 1.0 * self.bnz, self.leakyrate * self.bnz))\n",
    "        return\n",
    "    \n",
    "    def LeakyReLU_prime(self):\n",
    "        np.copyto(self.dJdbnz, np.where(self.z > 0, 1.0 * self.dJdr, self.leakyrate * self.dJdr))\n",
    "        return\n",
    "    \n",
    "    def tanh(self):\n",
    "        np.copyto(self.r, (np.exp(self.zr) - np.exp(-self.zr)) / (np.exp(self.zr) + np.exp(-self.zr)))\n",
    "        return\n",
    "    \n",
    "    def tanh_prime(self):\n",
    "        np.copyto(self.dJdzr, (1.0 - self.r ** 2))\n",
    "        return\n",
    "    \n",
    "    def swish(self):\n",
    "        np.copyto(self.zr, self.z * self.sigmoid(self.z))\n",
    "        return\n",
    "    \n",
    "    def swish_prime(self):\n",
    "        np.copyto(self.dJdz,\n",
    "                  self.zr + self.sigmoid(self.z) * (1.0 - self.zr))\n",
    "        return\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    # Batch normalization\n",
    "    # (zr, gamma, beta) --> r\n",
    "    def batch_norm(self, eps=0.01):  \n",
    "        \n",
    "        x = self.z\n",
    "        bs = x.shape[0]\n",
    "        mu = 1.0/bs * np.sum(x, axis=0)[None, :, :, :]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=0)[None, :, :, :]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        \n",
    "        np.copyto(self.bnz,\n",
    "                  self.gamma * xhat + self.beta)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # (dJdr, zr, gamma) --> (dJdbeta, dJdgamma, dJdzr)\n",
    "    def batch_norm_prime(self, eps = 0.01):\n",
    "        \n",
    "        x = self.z\n",
    "        bs = x.shape[0]\n",
    "        mu = 1.0/bs * np.sum(x, axis=0)[None, :, :, :]\n",
    "        u = x - mu\n",
    "        sigma2 = 1.0/bs * np.sum(u ** 2, axis=0)[None, :, :, :]\n",
    "        q = sigma2 + eps\n",
    "        v = np.sqrt(q)\n",
    "        xhat = u / v\n",
    "        \n",
    "        self.dJdbeta = np.mean(self.dJdbnz, axis=0)[None, :, :, :]\n",
    "        self.dJdgamma = np.mean(self.dJdbnz * xhat, axis=0)[None, :, :, :]\n",
    "        self.dJdz = (1.0 - 1.0/bs) * (1.0/v - u**2/v**3/bs) * self.gamma * self.dJdbnz\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def batch_norm_pass(self, eps=0.01):\n",
    "        \n",
    "        np.copyto(self.r, self.zr)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def batch_norm_prime_pass(self, eps = 0.01):\n",
    "        \n",
    "        np.copyto(self.dJdzr, self.dJdr)     \n",
    "        return   \n",
    "    \n",
    "    # Maximum Pooling\n",
    "    # r --> (ao, mp_trace)\n",
    "    def max_pool(self):  \n",
    "        (r_bs, r_ch, r_rows, r_cols) = self.r.shape\n",
    "        (a_bs, a_ch, a_rows, a_cols) = self.ao.shape\n",
    "        self.mp_trace.fill(0) # record from which is the maximum so we can backprop\n",
    "        \n",
    "        for b in range(a_bs):\n",
    "            for c in range(a_ch):\n",
    "                for i in range(a_rows):\n",
    "                    for j in range(a_cols):\n",
    "                        pool_src2d = self.r[b,\n",
    "                                            c,\n",
    "                                            i*self.mph:(i+1)*self.mph,\n",
    "                                            j*self.mpw:(j+1)*self.mpw] \n",
    "                        self.ao[b, c, i, j] = np.max(pool_src2d)\n",
    "                        max_pos = np.unravel_index(np.argmax(pool_src2d), \n",
    "                                                   np.shape(pool_src2d))\n",
    "                        self.mp_trace[b, c, i*self.mph+max_pos[0], j*self.mpw+max_pos[1]] = 1\n",
    "        return\n",
    "    \n",
    "    def max_pool_prime(self):\n",
    "        (r_bs, r_ch, r_rows, r_cols) = self.dJdr.shape\n",
    "        \n",
    "        for b in range(r_bs):\n",
    "            for c in range(r_ch):\n",
    "                for i in range(r_rows):\n",
    "                    for j in range(r_cols):\n",
    "                        self.dJdr[b, c, i, j] = (self.dJdao[b, c, i//self.mph, j//self.mpw] \n",
    "                                                  if self.mp_trace[b, c, i, j] == 1\n",
    "                                                  else 0.0)                       \n",
    "        return\n",
    "    \n",
    "    def forward(self):\n",
    "        \n",
    "        self.conv2d()  # ai --> z\n",
    "        self.batch_norm()  # z --> bnz\n",
    "        self.LeakyReLU()  # bnz --> r\n",
    "        self.max_pool()  # r --> ao\n",
    "        return\n",
    "    \n",
    "    def backprop(self):\n",
    "        self.max_pool_prime() # dJdao --> dJdr\n",
    "        self.LeakyReLU_prime() # dJdr --> dJdbnz\n",
    "        self.batch_norm_prime() # dJdnz --> dJdz\n",
    "        self.conv2d_prime() # dJdz --> dJdai\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def update(self, lr, eta):\n",
    "        # Update W, b, gamma, beta with Weight Decay\n",
    "        self.W = (1.0 - eta * lr) * self.W - lr * self.dJdW\n",
    "        self.b = (1.0 - eta * lr) * self.b - lr * self.dJdb\n",
    "        self.gamma = (1.0 - eta * lr) * self.gamma - lr * self.dJdgamma\n",
    "        self.beta = (1.0 - eta * lr) * self.beta - lr * self.beta\n",
    "        return\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Convolutional Neural Network (CNN) Consists of a number of conv layers followed by\n",
    "# a number of fully-connected layers\n",
    "class CNN(object):\n",
    "    def __init__(self,\n",
    "                input_data_spec = [10, 3, 28, 28],\n",
    "                conv_layer_spec = [{\"k_num\" : 2,\n",
    "                                    \"k_h\" : 3,\n",
    "                                    \"k_w\" : 3,\n",
    "                                    \"stride\" : 1,\n",
    "                                    \"zp\" : 0,\n",
    "                                    \"mph\" : 2,\n",
    "                                    \"mpw\" :2}],\n",
    "                 fc_layer_spec = [100, 50, 10]):\n",
    "        \n",
    "        # A list of conv layers\n",
    "        self.c_net = []\n",
    "        for i in range(len(conv_layer_spec)):\n",
    "            self.c_net.append(Conv_Layer(bs = input_data_spec[0],\n",
    "                                  i_ch = input_data_spec[1],\n",
    "                                  i_h = input_data_spec[2],\n",
    "                                  i_w = input_data_spec[3],\n",
    "                                  k_num = conv_layer_spec[i]['k_num'],\n",
    "                                  k_h = conv_layer_spec[i]['k_h'],\n",
    "                                  k_w = conv_layer_spec[i]['k_w'],\n",
    "                                  stride = conv_layer_spec[i]['stride'],\n",
    "                                  zp = conv_layer_spec[i]['zp'],\n",
    "                                  mph = conv_layer_spec[i]['mph'],\n",
    "                                  mpw = conv_layer_spec[i]['mpw']))\n",
    "            input_data_spec = list(self.c_net[i].ao.shape)\n",
    "                       \n",
    "        (bs, ch, r, c) = self.c_net[-1].ao.shape # Shape of Last Conv Layer's Output Feature Map\n",
    "        fc_layer_spec[0] = ch*r*c\n",
    "        self.fc_net = MLP(fc_layer_spec, BatchSize=input_data_spec[0])\n",
    "        return\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \n",
    "        # Forward through Conv Layers\n",
    "        for i in range(len(self.c_net)):\n",
    "            if i == 0:\n",
    "                np.copyto(self.c_net[i].ai, input_data)\n",
    "            else:\n",
    "                np.copyto(self.c_net[i].ai, self.c_net[i-1].ao)\n",
    "            self.c_net[i].forward()\n",
    "            \n",
    "        # Flatten the last output feature map of the conv layer for feeding to the MLP    \n",
    "        input_to_fc = np.copy(self.c_net[-1].ao.reshape(self.c_net[-1].ao.shape[0], -1).T)\n",
    "        \n",
    "        # Forward through the MLP ( anumber of fully connected layers\n",
    "        self.fc_net.forward(input_to_fc)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def backprop(self):\n",
    "        \n",
    "        # Backprop MLP\n",
    "        self.fc_net.backprop()\n",
    "        \n",
    "        # Transfer the gradients from the MLB to the conv layers\n",
    "        np.copyto(self.c_net[-1].dJdao, \n",
    "                  self.fc_net.net[0]['dJdi'].T.reshape(self.c_net[-1].dJdao.shape))\n",
    "        \n",
    "        # Backprop Conv layers\n",
    "        for i in range(len(self.c_net)-1, -1, -1):\n",
    "            self.c_net[i].backprop()\n",
    "            if i > 0:\n",
    "                np.copyto(self.c_net[i-1].dJdao, self.c_net[i].dJdai)\n",
    "        return\n",
    "    \n",
    "    def update(self, lr, eta):\n",
    "        \n",
    "        # Weight update of conv layers\n",
    "        for i in range(len(self.c_net)):\n",
    "            self.c_net[i].update(lr, eta)\n",
    "            \n",
    "        # Weight Update of MLP\n",
    "        self.fc_net.update(lr, eta)\n",
    "        \n",
    "        return\n",
    "\n",
    "    # Find learning rate\n",
    "    def proper_lr(self):\n",
    "        max_W = 0.0\n",
    "        max_dJdW = 0.0\n",
    "        for conv_net in self.c_net:\n",
    "            max_W = max(max_W, np.max(np.abs(conv_net.W)))\n",
    "            max_dJdW = max(max_dJdW, np.max(np.abs(conv_net.dJdW)))\n",
    "        for fc_net in self.fc_net.net[1:]:\n",
    "            max_W = max(max_W, np.max(np.abs(fc_net['W'])))\n",
    "            max_dJdW = max(max_dJdW, np.max(np.abs(fc_net['dJdW'])))\n",
    "        return min(1.0, 0.1 * max_W / max_dJdW)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    # Train a CNN end-to-end\n",
    "    def train(self, train_x, train_y, val_x, val_y, epoch_count, lr, eta):\n",
    "        \n",
    "        for e in range(epoch_count):\n",
    "            # Randomly shuffle the training data at the begining of an epoch\n",
    "            shuffle = np.arange(train_x.shape[0])\n",
    "            np.random.shuffle(shuffle)\n",
    "            train_x_s = train_x[shuffle]\n",
    "            train_y_s = train_y[shuffle]\n",
    "            bs = self.fc_net.bs\n",
    "            \n",
    "            for i in range(train_x_s.shape[0]//bs):\n",
    "                x = train_x_s[i*bs:(i+1)*bs, :, : ,:]\n",
    "                y = train_y_s[i*bs:(i+1)*bs]    \n",
    "                self.forward(x)\n",
    "                self.fc_net.loss(y, eta)\n",
    "                self.backprop()\n",
    "                self.update(lr, eta)\n",
    "                \n",
    "                print (\"\\nEpoch\", e, \"Batch\", i, \"J=\", self.fc_net.J[-1], \n",
    "                       \"Error Rate=\", np.count_nonzero(np.array(y-self.fc_net.yhat)) / float(len(y)))\n",
    "                      # \"\\nyhat=\", self.fc_net.yhat, \"\\n   y=\", y)\n",
    "                #print (\"\\nProper lr=\", lr)\n",
    "                print (\"\\nMax abs(a) of Last MLP Layer=\", np.max(np.abs(self.fc_net.net[-1]['a'])), \n",
    "                       \"\\nMax abs(dJda) of Last MLP Layer=\", np.max(np.abs(self.fc_net.net[-1]['dJda'])),\n",
    "                      \"\\nMax abs(W) of Last MLP Layer=\", np.max(np.abs(self.fc_net.net[1]['W'])), \"\\n\")\n",
    "            # Validation\n",
    "            for i in range(val_x.shape[0]//bs):\n",
    "                x = val_x[i*bs:(i+1)*bs, :, : ,:]\n",
    "                y = val_y[i*bs:(i+1)*bs]    \n",
    "                self.forward(x)\n",
    "                self.fc_net.loss_val(y, eta)\n",
    "                \n",
    "        return             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"mnist_train_60000.csv\", 'r')\n",
    "a = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# f = figure(figsize=(15,15));\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "x = []\n",
    "y = []\n",
    "count=1\n",
    "for line in a:\n",
    "    linebits = line.split(',')\n",
    "    x_line = [int(linebits[i]) for i in range(len(linebits))]\n",
    "    x.append(x_line[1:])\n",
    "    y.append(x_line[0])\n",
    "    imarray = np.asfarray(linebits[1:]).reshape((28,28))\n",
    "    count += 1\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "train_60000_x_MLP = np.array(x).T\n",
    "train_60000_y_MLP = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layers = (784, 50, 10)\n",
    "BatchSize = 60000\n",
    "EpochCount = 100\n",
    "LearningRate = 0.1\n",
    "Eta = 0.00001\n",
    "\n",
    "mlp60000 = MLP(Layers, BatchSize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp60000.train(train_60000_x_MLP, train_60000_y_MLP, EpochCount*2, LearningRate, Eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(target,predict):\n",
    "    correct = 0\n",
    "    for i in range(len(target)):\n",
    "        if (target[i] - predict[i]) == 0:\n",
    "            correct += 1\n",
    "    return correct/len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.5819166666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = \", acc(train_60000_y_MLP, mlp60000.yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time is     13.17792    seconds\n"
     ]
    }
   ],
   "source": [
    "# Read in 60000 digits\n",
    "\n",
    "\n",
    "f = open(\"mnist_train_60000.csv\", 'r')\n",
    "a = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# f = figure(figsize=(15,15));\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "x = []\n",
    "y = []\n",
    "count=1\n",
    "for line in a:\n",
    "    linebits = line.split(',')\n",
    "    x_line = [int(linebits[i]) for i in range(len(linebits))]\n",
    "    x.append(x_line[1:])\n",
    "    y.append(x_line[0])\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "train_60000_x = np.clip(np.array(x), 0, 1).reshape(60000, 1, 28, 28)\n",
    "train_60000_y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN identical to LeNet\n",
    "BatchSize = 250\n",
    "EpochCount = 1\n",
    "LearningRate = 0.01\n",
    "Eta = 0.001\n",
    "\n",
    "input_data_spec = [BatchSize, 1, 28, 28] # [BatchSize, number of input chanels, Width, Height]\n",
    "conv_layer_spec = [{\"k_num\" : 6, \"k_h\" : 5, \"k_w\" : 5, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2},\n",
    "                   {\"k_num\" : 16, \"k_h\" : 3, \"k_w\" : 3, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2}]\n",
    "fc_layer_spec = [128,128,128,10]\n",
    "\n",
    "cnn60000 = CNN(input_data_spec=input_data_spec, \n",
    "          conv_layer_spec=conv_layer_spec, \n",
    "          fc_layer_spec=fc_layer_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Batch 0 J= 2.2932751539117557 Error Rate= 0.748\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.969989157424951 \n",
      "Max abs(dJda) of Last MLP Layer= 1.8706379846284926 \n",
      "Max abs(W) of Last MLP Layer= 4.892952844692668 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 1 J= 2.308966052643724 Error Rate= 0.776\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.370730981686749 \n",
      "Max abs(dJda) of Last MLP Layer= 1.691641809786176 \n",
      "Max abs(W) of Last MLP Layer= 4.892900423346518 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 2 J= 2.2010429346513503 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.2643205738776055 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7030184317995802 \n",
      "Max abs(W) of Last MLP Layer= 4.89285099040263 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 3 J= 2.2153309843165836 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.400514910556581 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5194634523368546 \n",
      "Max abs(W) of Last MLP Layer= 4.892793234295298 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 4 J= 2.219598594676271 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.166658149269281 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7206444316966119 \n",
      "Max abs(W) of Last MLP Layer= 4.892733188151669 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 5 J= 2.237563181041487 Error Rate= 0.76\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.462569651722716 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3726163223272538 \n",
      "Max abs(W) of Last MLP Layer= 4.892675148677325 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 6 J= 2.1578725746056793 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.376314465325465 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4993574965215677 \n",
      "Max abs(W) of Last MLP Layer= 4.892601954405391 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 7 J= 2.1878149591137284 Error Rate= 0.716\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.649498067611642 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1980493346613512 \n",
      "Max abs(W) of Last MLP Layer= 4.892541710449174 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 8 J= 2.0916926731185996 Error Rate= 0.66\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.896838775386382 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4873349926911121 \n",
      "Max abs(W) of Last MLP Layer= 4.892485134345396 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 9 J= 2.1177946333077178 Error Rate= 0.688\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.6539596400977965 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5628743345247393 \n",
      "Max abs(W) of Last MLP Layer= 4.892435437364876 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 10 J= 2.24528084468201 Error Rate= 0.716\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.566689629716823 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5577149924834461 \n",
      "Max abs(W) of Last MLP Layer= 4.892378952138735 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 11 J= 2.2481870927209986 Error Rate= 0.768\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.779141863925331 \n",
      "Max abs(dJda) of Last MLP Layer= 1.717546470882168 \n",
      "Max abs(W) of Last MLP Layer= 4.8923121932991345 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 12 J= 2.1720737107861927 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.256738191154806 \n",
      "Max abs(dJda) of Last MLP Layer= 1.478748277668525 \n",
      "Max abs(W) of Last MLP Layer= 4.8922498361727795 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 13 J= 2.1952191710065474 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.915146447067692 \n",
      "Max abs(dJda) of Last MLP Layer= 1.344375902871579 \n",
      "Max abs(W) of Last MLP Layer= 4.892182136928925 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 14 J= 2.2828643230472636 Error Rate= 0.768\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.263462102042275 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3793404506542974 \n",
      "Max abs(W) of Last MLP Layer= 4.8921314750637 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 15 J= 2.153458605811961 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.951390842003769 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4777172441556936 \n",
      "Max abs(W) of Last MLP Layer= 4.89207200479377 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 16 J= 2.2285657081485204 Error Rate= 0.732\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.016467855376279 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4198165182875777 \n",
      "Max abs(W) of Last MLP Layer= 4.892014777571576 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 17 J= 2.0521994314723897 Error Rate= 0.652\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.114282850087112 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1943490822724858 \n",
      "Max abs(W) of Last MLP Layer= 4.891951655310663 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 18 J= 2.060004824766354 Error Rate= 0.66\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.457516968038763 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4054018060330098 \n",
      "Max abs(W) of Last MLP Layer= 4.891886708234868 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 19 J= 2.2414550522637073 Error Rate= 0.76\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.0991340311390445 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2181330238567971 \n",
      "Max abs(W) of Last MLP Layer= 4.891832764148234 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 20 J= 2.2484099082124187 Error Rate= 0.752\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.582527936598122 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3866951027901704 \n",
      "Max abs(W) of Last MLP Layer= 4.891780119821735 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 21 J= 2.1269774171436975 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.868809715389596 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3828154088568747 \n",
      "Max abs(W) of Last MLP Layer= 4.891714860234317 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 22 J= 2.1694403058975746 Error Rate= 0.732\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.412207960113652 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7996859628708899 \n",
      "Max abs(W) of Last MLP Layer= 4.891647441259347 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 23 J= 2.1595914621424015 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.138804120795542 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2358145734758206 \n",
      "Max abs(W) of Last MLP Layer= 4.8915911022698895 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 24 J= 2.1111040666433993 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.3022975851256415 \n",
      "Max abs(dJda) of Last MLP Layer= 1.187423038099339 \n",
      "Max abs(W) of Last MLP Layer= 4.8915473159990315 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 25 J= 2.169015412762866 Error Rate= 0.724\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.957535695769887 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3401843593398284 \n",
      "Max abs(W) of Last MLP Layer= 4.89148934813152 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 26 J= 2.1982230356347605 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.319874921407018 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3168170839703435 \n",
      "Max abs(W) of Last MLP Layer= 4.891441058824264 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 27 J= 2.1958636739616697 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.395639825678316 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2876772727206063 \n",
      "Max abs(W) of Last MLP Layer= 4.89137238703039 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 28 J= 2.1727934798883037 Error Rate= 0.716\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.4737146610846015 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3457593730031105 \n",
      "Max abs(W) of Last MLP Layer= 4.891298303786772 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 29 J= 2.158479537557689 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.2829880994529335 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4549211514335543 \n",
      "Max abs(W) of Last MLP Layer= 4.8912359323354275 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 30 J= 2.096516279883552 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.342482030767397 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5120217432447232 \n",
      "Max abs(W) of Last MLP Layer= 4.891182757323707 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 31 J= 2.1250035273589214 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.17654222040071 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5913032004273324 \n",
      "Max abs(W) of Last MLP Layer= 4.891134012964304 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 32 J= 2.218322429848782 Error Rate= 0.736\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.298717916991127 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4915981508038454 \n",
      "Max abs(W) of Last MLP Layer= 4.891083042794221 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 33 J= 2.092296812177424 Error Rate= 0.66\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.748527876880496 \n",
      "Max abs(dJda) of Last MLP Layer= 1.8673515310021216 \n",
      "Max abs(W) of Last MLP Layer= 4.891013532064967 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 34 J= 2.1751413640624655 Error Rate= 0.716\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.343737855730041 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3930021033525217 \n",
      "Max abs(W) of Last MLP Layer= 4.89094403829581 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 35 J= 2.10328244469223 Error Rate= 0.656\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.136257110820311 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5590341769762885 \n",
      "Max abs(W) of Last MLP Layer= 4.890883367737601 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 36 J= 2.191975434934378 Error Rate= 0.736\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.405927352002976 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2866955844789338 \n",
      "Max abs(W) of Last MLP Layer= 4.890823022979563 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 37 J= 2.140987966136314 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.334121665309783 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5302170396796915 \n",
      "Max abs(W) of Last MLP Layer= 4.890762293695498 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 38 J= 2.1604882486499624 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.866097469170024 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3815612839199831 \n",
      "Max abs(W) of Last MLP Layer= 4.890694281736225 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 39 J= 2.229188889747809 Error Rate= 0.728\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.326735836820436 \n",
      "Max abs(dJda) of Last MLP Layer= 1.315996300943062 \n",
      "Max abs(W) of Last MLP Layer= 4.890635287204432 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Batch 40 J= 2.1860921515616205 Error Rate= 0.688\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.854820072143543 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5626770786857564 \n",
      "Max abs(W) of Last MLP Layer= 4.89057357329037 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 41 J= 2.1151058675234378 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.117689380347505 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3439824887092873 \n",
      "Max abs(W) of Last MLP Layer= 4.890523687221976 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 42 J= 2.1614408262687794 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.313895694114334 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5273935982153648 \n",
      "Max abs(W) of Last MLP Layer= 4.890467409205957 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 43 J= 2.1494794240095523 Error Rate= 0.688\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.614997209256595 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4199652639731233 \n",
      "Max abs(W) of Last MLP Layer= 4.890405625137235 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 44 J= 2.095143566430158 Error Rate= 0.66\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.330965404801272 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3666762676990878 \n",
      "Max abs(W) of Last MLP Layer= 4.890342693186573 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 45 J= 2.3215472094737164 Error Rate= 0.788\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.916719888652272 \n",
      "Max abs(dJda) of Last MLP Layer= 1.8718615085500399 \n",
      "Max abs(W) of Last MLP Layer= 4.89026682516233 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 46 J= 2.3135091565011185 Error Rate= 0.736\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.330076666696477 \n",
      "Max abs(dJda) of Last MLP Layer= 1.797491729575098 \n",
      "Max abs(W) of Last MLP Layer= 4.890219944946984 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 47 J= 2.2578583732091957 Error Rate= 0.744\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.085558183465357 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7891610008563754 \n",
      "Max abs(W) of Last MLP Layer= 4.8901648861034275 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 48 J= 2.2134286137361916 Error Rate= 0.744\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.137277601243679 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4045072233599758 \n",
      "Max abs(W) of Last MLP Layer= 4.890101380277045 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 49 J= 2.027694937274676 Error Rate= 0.664\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.34086925047583 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3434447897760866 \n",
      "Max abs(W) of Last MLP Layer= 4.890048983923903 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 50 J= 2.167192212550471 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.544841499606661 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4276438991355938 \n",
      "Max abs(W) of Last MLP Layer= 4.889985969750284 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 51 J= 2.111408674171072 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.768695678788767 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1699772257591912 \n",
      "Max abs(W) of Last MLP Layer= 4.88991742255323 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 52 J= 2.190902169665958 Error Rate= 0.724\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.34798677214332 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2861945603903104 \n",
      "Max abs(W) of Last MLP Layer= 4.889863091417683 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 53 J= 2.1192141560247006 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.767863018641152 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4688838250792062 \n",
      "Max abs(W) of Last MLP Layer= 4.8897978746681465 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 54 J= 2.1044243036727854 Error Rate= 0.66\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.101990769339471 \n",
      "Max abs(dJda) of Last MLP Layer= 1.367003193010754 \n",
      "Max abs(W) of Last MLP Layer= 4.8897392751647075 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 55 J= 2.2490887468863083 Error Rate= 0.732\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.06316346788698 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6453250739605494 \n",
      "Max abs(W) of Last MLP Layer= 4.889674307600285 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 56 J= 2.15059533098401 Error Rate= 0.724\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.409444478939844 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4862087823110388 \n",
      "Max abs(W) of Last MLP Layer= 4.889608414449866 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 57 J= 2.100662337086507 Error Rate= 0.656\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.315999336542844 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7799571028713528 \n",
      "Max abs(W) of Last MLP Layer= 4.889544055024856 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 58 J= 2.2800585226575554 Error Rate= 0.764\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.7902888660405925 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1900647854957795 \n",
      "Max abs(W) of Last MLP Layer= 4.889490499160028 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 59 J= 2.153076095681877 Error Rate= 0.688\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.1300362942007265 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3707347830686372 \n",
      "Max abs(W) of Last MLP Layer= 4.8894372812268525 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 60 J= 2.009750377811166 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.699046180436111 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2724817211613164 \n",
      "Max abs(W) of Last MLP Layer= 4.889377528083535 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 61 J= 2.1896217619162575 Error Rate= 0.744\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.6476017257312465 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3638555081044126 \n",
      "Max abs(W) of Last MLP Layer= 4.889330318631996 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 62 J= 2.175350353565018 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.888868407462737 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2749522993693143 \n",
      "Max abs(W) of Last MLP Layer= 4.889275492197819 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 63 J= 2.2514457348850336 Error Rate= 0.728\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.477378932528964 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5372820667691518 \n",
      "Max abs(W) of Last MLP Layer= 4.8892087994863225 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 64 J= 2.15094077334441 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.103983079592271 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3515944009982532 \n",
      "Max abs(W) of Last MLP Layer= 4.889157424838086 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 65 J= 2.2634066283451233 Error Rate= 0.744\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.8501958873028475 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7214370164263588 \n",
      "Max abs(W) of Last MLP Layer= 4.889098570794668 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 66 J= 2.138939521179879 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.475905984344834 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3183470725798982 \n",
      "Max abs(W) of Last MLP Layer= 4.889046628017991 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 67 J= 2.1304461852976653 Error Rate= 0.684\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.120132131520428 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3682719185655576 \n",
      "Max abs(W) of Last MLP Layer= 4.8889810027247975 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 68 J= 2.267587626621051 Error Rate= 0.724\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.623823693826029 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6040681289394252 \n",
      "Max abs(W) of Last MLP Layer= 4.888919019956509 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 69 J= 2.1807684164782164 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.651943027059817 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2752122665484966 \n",
      "Max abs(W) of Last MLP Layer= 4.888854859173454 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 70 J= 2.1532045849617427 Error Rate= 0.728\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.844955190442578 \n",
      "Max abs(dJda) of Last MLP Layer= 1.398067810665206 \n",
      "Max abs(W) of Last MLP Layer= 4.888803172864989 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 71 J= 2.2098806104514286 Error Rate= 0.744\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.179248844402628 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5268045338010494 \n",
      "Max abs(W) of Last MLP Layer= 4.888733978320831 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 72 J= 2.170892501571154 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.514394637297351 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6078835560757438 \n",
      "Max abs(W) of Last MLP Layer= 4.8886704416684506 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 73 J= 2.08203581174055 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.6797230001494885 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3875187003408933 \n",
      "Max abs(W) of Last MLP Layer= 4.888624652994045 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 74 J= 2.1581594634330212 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.046458531220484 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5932918585425762 \n",
      "Max abs(W) of Last MLP Layer= 4.888556264141442 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 75 J= 2.0926752675349514 Error Rate= 0.688\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.998370339365698 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4790052074978997 \n",
      "Max abs(W) of Last MLP Layer= 4.888492023405752 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 76 J= 2.2035446395075113 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.433650319410829 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5309996853868055 \n",
      "Max abs(W) of Last MLP Layer= 4.888434037928748 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 77 J= 2.156252261465972 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.674398827943558 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6130583171613353 \n",
      "Max abs(W) of Last MLP Layer= 4.888381004144109 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 78 J= 2.2212877178228707 Error Rate= 0.716\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.668171248675578 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4122203513466092 \n",
      "Max abs(W) of Last MLP Layer= 4.888316927662788 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Batch 79 J= 2.191383617312427 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.596315340463782 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7466938918369745 \n",
      "Max abs(W) of Last MLP Layer= 4.88826097954313 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 80 J= 2.2172552210979473 Error Rate= 0.74\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.697065692434295 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4424074752273792 \n",
      "Max abs(W) of Last MLP Layer= 4.8881992012560636 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 81 J= 2.3173062192028877 Error Rate= 0.752\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.773497940290929 \n",
      "Max abs(dJda) of Last MLP Layer= 1.396257457743042 \n",
      "Max abs(W) of Last MLP Layer= 4.888145105713163 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 82 J= 2.273510727289783 Error Rate= 0.732\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.225684455759162 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5187529314528225 \n",
      "Max abs(W) of Last MLP Layer= 4.888093954610187 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 83 J= 2.1420769043266974 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.354804751463109 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6361101884830687 \n",
      "Max abs(W) of Last MLP Layer= 4.88803160571566 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 84 J= 2.2049207212612476 Error Rate= 0.732\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.030989361127242 \n",
      "Max abs(dJda) of Last MLP Layer= 1.475850581973706 \n",
      "Max abs(W) of Last MLP Layer= 4.887982285930623 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 85 J= 2.174782621170581 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.383125593172845 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5278832610500364 \n",
      "Max abs(W) of Last MLP Layer= 4.8879260544934295 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 86 J= 2.0862008798656753 Error Rate= 0.636\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.097113520527121 \n",
      "Max abs(dJda) of Last MLP Layer= 1.541506878071238 \n",
      "Max abs(W) of Last MLP Layer= 4.887861969831178 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 87 J= 2.0052206017012972 Error Rate= 0.656\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.025536426530033 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4546859836068307 \n",
      "Max abs(W) of Last MLP Layer= 4.887814571794765 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 88 J= 2.058387090146719 Error Rate= 0.684\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.763643557007541 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1911798239334737 \n",
      "Max abs(W) of Last MLP Layer= 4.887759189508998 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 89 J= 2.128096174436571 Error Rate= 0.688\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.452658265856058 \n",
      "Max abs(dJda) of Last MLP Layer= 1.32437245745978 \n",
      "Max abs(W) of Last MLP Layer= 4.88770244614253 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 90 J= 2.085791066562039 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.087071363181133 \n",
      "Max abs(dJda) of Last MLP Layer= 1.297130658223991 \n",
      "Max abs(W) of Last MLP Layer= 4.887644658279225 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 91 J= 2.0384998124342357 Error Rate= 0.656\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.853036126006248 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3752273038807656 \n",
      "Max abs(W) of Last MLP Layer= 4.887596096497717 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 92 J= 2.2290587208138506 Error Rate= 0.728\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.433774659578833 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3170657069145424 \n",
      "Max abs(W) of Last MLP Layer= 4.88752791060242 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 93 J= 2.0829675886040144 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.398746676921281 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3160526682125402 \n",
      "Max abs(W) of Last MLP Layer= 4.88747133518909 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 94 J= 2.1744326574187918 Error Rate= 0.668\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.821382297324455 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3536084512996225 \n",
      "Max abs(W) of Last MLP Layer= 4.887414012113426 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 95 J= 2.0934960341321887 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.548491842800978 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2921346510924776 \n",
      "Max abs(W) of Last MLP Layer= 4.887361749415487 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 96 J= 2.1219645445592277 Error Rate= 0.716\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.34438641357365 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5035928110441406 \n",
      "Max abs(W) of Last MLP Layer= 4.887308948241349 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 97 J= 2.0662806035743992 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.233050719646246 \n",
      "Max abs(dJda) of Last MLP Layer= 1.634200253213735 \n",
      "Max abs(W) of Last MLP Layer= 4.887241656860163 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 98 J= 2.094210774351856 Error Rate= 0.66\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.962232506002898 \n",
      "Max abs(dJda) of Last MLP Layer= 1.650530335023809 \n",
      "Max abs(W) of Last MLP Layer= 4.887173985024336 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 99 J= 2.148930871222425 Error Rate= 0.724\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.421797451729649 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5885368041802514 \n",
      "Max abs(W) of Last MLP Layer= 4.88710992868045 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 100 J= 2.125022118278713 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.7772790072931555 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4415669172677004 \n",
      "Max abs(W) of Last MLP Layer= 4.887050404178873 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 101 J= 2.01697375933263 Error Rate= 0.652\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.697335815709957 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4221708903860195 \n",
      "Max abs(W) of Last MLP Layer= 4.886982073902623 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 102 J= 2.118751094588635 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.0323009723300585 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5133648543628988 \n",
      "Max abs(W) of Last MLP Layer= 4.886915685917213 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 103 J= 2.111712786702289 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.423250901361184 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2205217921857427 \n",
      "Max abs(W) of Last MLP Layer= 4.886862196652928 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 104 J= 2.046964927563133 Error Rate= 0.632\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.927880310717793 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1056585490426443 \n",
      "Max abs(W) of Last MLP Layer= 4.88680654557467 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 105 J= 2.1487760398617364 Error Rate= 0.688\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.886804966291311 \n",
      "Max abs(dJda) of Last MLP Layer= 1.343775584099106 \n",
      "Max abs(W) of Last MLP Layer= 4.886746356508202 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 106 J= 2.005134552121358 Error Rate= 0.648\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.770973305268552 \n",
      "Max abs(dJda) of Last MLP Layer= 1.077681644866633 \n",
      "Max abs(W) of Last MLP Layer= 4.886690246924204 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 107 J= 2.163976662130545 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.139512185990776 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7609935902756968 \n",
      "Max abs(W) of Last MLP Layer= 4.886629934912886 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 108 J= 2.1043167030638488 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.786042762543889 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6277966287230208 \n",
      "Max abs(W) of Last MLP Layer= 4.886571326888074 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 109 J= 2.059295631233326 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.337084924928049 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1208239854828705 \n",
      "Max abs(W) of Last MLP Layer= 4.886514269063797 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 110 J= 2.1184965434584817 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.8859365503775445 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5336551721376777 \n",
      "Max abs(W) of Last MLP Layer= 4.886458503922906 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 111 J= 2.119717009657064 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.806642963818111 \n",
      "Max abs(dJda) of Last MLP Layer= 1.309672331084253 \n",
      "Max abs(W) of Last MLP Layer= 4.88639665323738 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 112 J= 1.931444316908069 Error Rate= 0.66\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.794777301624471 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2274840042297541 \n",
      "Max abs(W) of Last MLP Layer= 4.886331568051661 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 113 J= 2.0574460699482273 Error Rate= 0.664\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.3226394735081035 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2858607805999553 \n",
      "Max abs(W) of Last MLP Layer= 4.886266269810993 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 114 J= 2.1337646027748733 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.44811020831795 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5860401104429145 \n",
      "Max abs(W) of Last MLP Layer= 4.886218597476446 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 115 J= 2.0737553363410735 Error Rate= 0.652\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.9136395314452335 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5046454195494046 \n",
      "Max abs(W) of Last MLP Layer= 4.886165464421413 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 116 J= 2.0143561779733425 Error Rate= 0.652\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.11619039571916 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1737280177013623 \n",
      "Max abs(W) of Last MLP Layer= 4.886114945707676 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 117 J= 2.109869956962672 Error Rate= 0.668\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.619513017975215 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4431181722656792 \n",
      "Max abs(W) of Last MLP Layer= 4.8860506636836085 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Batch 118 J= 2.1717713182395606 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.451289669289849 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5486189037620157 \n",
      "Max abs(W) of Last MLP Layer= 4.885989727136165 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 119 J= 2.1158391975945317 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.369222497861305 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3704575754903612 \n",
      "Max abs(W) of Last MLP Layer= 4.885933632969744 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 120 J= 2.1619475454329096 Error Rate= 0.732\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.602864088096582 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6113579533160796 \n",
      "Max abs(W) of Last MLP Layer= 4.885868875372322 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 121 J= 2.1598389668084854 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.647755914861606 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4753495998185304 \n",
      "Max abs(W) of Last MLP Layer= 4.885811631925684 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 122 J= 2.1249291344458094 Error Rate= 0.724\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.042797142209571 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3720327591527095 \n",
      "Max abs(W) of Last MLP Layer= 4.885762972194526 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 123 J= 2.0575748065227213 Error Rate= 0.644\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.375193462724302 \n",
      "Max abs(dJda) of Last MLP Layer= 1.166773757986391 \n",
      "Max abs(W) of Last MLP Layer= 4.885708161342219 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 124 J= 1.9861358224928374 Error Rate= 0.644\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.027743064988396 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3831783718316983 \n",
      "Max abs(W) of Last MLP Layer= 4.885648073834062 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 125 J= 2.1042939508062677 Error Rate= 0.664\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.148069266952902 \n",
      "Max abs(dJda) of Last MLP Layer= 1.780725757772375 \n",
      "Max abs(W) of Last MLP Layer= 4.885584254792417 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 126 J= 2.1722861129419093 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.536289863120782 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5474022172572162 \n",
      "Max abs(W) of Last MLP Layer= 4.885517404982264 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 127 J= 2.1471887010054704 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.7884985121212065 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3964852589034333 \n",
      "Max abs(W) of Last MLP Layer= 4.885459928304998 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 128 J= 2.1259731612754114 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.5583065186480045 \n",
      "Max abs(dJda) of Last MLP Layer= 1.8348319862940063 \n",
      "Max abs(W) of Last MLP Layer= 4.885402069711714 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 129 J= 2.1422406379776797 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.510637107474451 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4615300837506937 \n",
      "Max abs(W) of Last MLP Layer= 4.88534604853111 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 130 J= 2.07415484938224 Error Rate= 0.652\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.103576780525098 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4444146508131168 \n",
      "Max abs(W) of Last MLP Layer= 4.885285228542206 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 131 J= 1.9768862455411083 Error Rate= 0.632\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.546765932879417 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5312457701411293 \n",
      "Max abs(W) of Last MLP Layer= 4.885236835247867 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 132 J= 2.076909100071568 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.96970996892661 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1653239134987652 \n",
      "Max abs(W) of Last MLP Layer= 4.885178774140166 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 133 J= 2.1307620260566225 Error Rate= 0.684\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.85958399200265 \n",
      "Max abs(dJda) of Last MLP Layer= 1.235174747857028 \n",
      "Max abs(W) of Last MLP Layer= 4.885121766463824 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 134 J= 2.087658007333026 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.093263208083187 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2423078180423968 \n",
      "Max abs(W) of Last MLP Layer= 4.885052669562337 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 135 J= 2.26854422431058 Error Rate= 0.744\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.933023449777189 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4640826836741496 \n",
      "Max abs(W) of Last MLP Layer= 4.8849879429926535 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 136 J= 2.101558201920673 Error Rate= 0.684\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.8644179567030585 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2126033104498655 \n",
      "Max abs(W) of Last MLP Layer= 4.884928919902082 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 137 J= 2.051958778007265 Error Rate= 0.668\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.20883252939146 \n",
      "Max abs(dJda) of Last MLP Layer= 1.348525770410058 \n",
      "Max abs(W) of Last MLP Layer= 4.884869137176558 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 138 J= 1.96822102951382 Error Rate= 0.628\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.7728729883743055 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1578087230405356 \n",
      "Max abs(W) of Last MLP Layer= 4.8848008297448695 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 139 J= 2.0775393769308126 Error Rate= 0.652\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.566528644319726 \n",
      "Max abs(dJda) of Last MLP Layer= 1.310107743302809 \n",
      "Max abs(W) of Last MLP Layer= 4.8847438838866095 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 140 J= 2.106901480978598 Error Rate= 0.684\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.886382584209988 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5827389646979955 \n",
      "Max abs(W) of Last MLP Layer= 4.884680112086983 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 141 J= 2.1124278412800956 Error Rate= 0.668\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.4035952333426405 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6204668210386504 \n",
      "Max abs(W) of Last MLP Layer= 4.884618896989112 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 142 J= 2.1820217257795695 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.281541474255216 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5809732262891585 \n",
      "Max abs(W) of Last MLP Layer= 4.884557705665999 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 143 J= 2.200593705137289 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.746763335650082 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5753030818932263 \n",
      "Max abs(W) of Last MLP Layer= 4.884498136458127 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 144 J= 2.1528539628325998 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.820830500587895 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2700829766767483 \n",
      "Max abs(W) of Last MLP Layer= 4.88443957364535 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 145 J= 2.1870450020467356 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.987256855380916 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5758072980839342 \n",
      "Max abs(W) of Last MLP Layer= 4.884396127850886 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 146 J= 2.0666046561376703 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.850256267993851 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2369318115954437 \n",
      "Max abs(W) of Last MLP Layer= 4.884335746647665 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 147 J= 2.04628572642585 Error Rate= 0.664\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.892083487449174 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1092831003638701 \n",
      "Max abs(W) of Last MLP Layer= 4.8842749170805275 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 148 J= 2.053581005786997 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.977956923074778 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3856301425112592 \n",
      "Max abs(W) of Last MLP Layer= 4.884210271867866 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 149 J= 2.22378575537578 Error Rate= 0.724\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.6538422658738945 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7449764318717538 \n",
      "Max abs(W) of Last MLP Layer= 4.88414910008576 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 150 J= 2.13396144606621 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.478257873473615 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5067773699141447 \n",
      "Max abs(W) of Last MLP Layer= 4.884085957884008 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 151 J= 2.108973056418189 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.080299874339565 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6691371901669672 \n",
      "Max abs(W) of Last MLP Layer= 4.884023635953824 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 152 J= 2.1509400710754036 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.742120682215887 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4862268041742395 \n",
      "Max abs(W) of Last MLP Layer= 4.88396338615296 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 153 J= 2.153827758014666 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.022071667902145 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4871513624383774 \n",
      "Max abs(W) of Last MLP Layer= 4.8839028698791935 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 154 J= 2.098600362311478 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.075186720189451 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4097131752220768 \n",
      "Max abs(W) of Last MLP Layer= 4.883852805360531 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 155 J= 2.2709575680050067 Error Rate= 0.732\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.296621591388141 \n",
      "Max abs(dJda) of Last MLP Layer= 1.504298449067509 \n",
      "Max abs(W) of Last MLP Layer= 4.883772093688604 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 156 J= 1.9721257945303878 Error Rate= 0.64\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.219730413253482 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2604183732019578 \n",
      "Max abs(W) of Last MLP Layer= 4.883712843066972 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Batch 157 J= 2.203310035559638 Error Rate= 0.748\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.915624539551208 \n",
      "Max abs(dJda) of Last MLP Layer= 1.633506331391922 \n",
      "Max abs(W) of Last MLP Layer= 4.88364653021209 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 158 J= 2.1088217040930894 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.436676778536992 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1990788162453452 \n",
      "Max abs(W) of Last MLP Layer= 4.88359618001176 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 159 J= 2.282842365193745 Error Rate= 0.784\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.725318877619375 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7714420981744947 \n",
      "Max abs(W) of Last MLP Layer= 4.883538100142895 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 160 J= 2.086042058914206 Error Rate= 0.688\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.133237794755155 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2063441122273044 \n",
      "Max abs(W) of Last MLP Layer= 4.883479626282704 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 161 J= 2.1599568028104867 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.103496538639799 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7427751081448424 \n",
      "Max abs(W) of Last MLP Layer= 4.883422092076209 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 162 J= 2.0951468384914165 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.086306642474613 \n",
      "Max abs(dJda) of Last MLP Layer= 1.798838048816694 \n",
      "Max abs(W) of Last MLP Layer= 4.883363376156729 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 163 J= 2.021710652951193 Error Rate= 0.648\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.041125634882047 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2631533335550704 \n",
      "Max abs(W) of Last MLP Layer= 4.88330984733877 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 164 J= 2.13596071977999 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.69291674995204 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5722205977091548 \n",
      "Max abs(W) of Last MLP Layer= 4.883261832838416 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 165 J= 2.087651765256468 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.472657014308856 \n",
      "Max abs(dJda) of Last MLP Layer= 1.842196610735279 \n",
      "Max abs(W) of Last MLP Layer= 4.8832063984321294 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 166 J= 2.101205531764472 Error Rate= 0.668\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.5872168680347505 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6979876140802506 \n",
      "Max abs(W) of Last MLP Layer= 4.883147731580618 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 167 J= 2.098752884067016 Error Rate= 0.684\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.590668376169385 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5728360379426298 \n",
      "Max abs(W) of Last MLP Layer= 4.883079144511542 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 168 J= 2.092359215241806 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.26810673996529 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3418072573697142 \n",
      "Max abs(W) of Last MLP Layer= 4.88303071699705 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 169 J= 2.1557580630168243 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.313806632104122 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2535797286472201 \n",
      "Max abs(W) of Last MLP Layer= 4.882962043876172 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 170 J= 2.1641214581572754 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.198073626862538 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3759137765925784 \n",
      "Max abs(W) of Last MLP Layer= 4.882901510884513 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 171 J= 2.0877139448508846 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.638196141339291 \n",
      "Max abs(dJda) of Last MLP Layer= 1.0483040105645196 \n",
      "Max abs(W) of Last MLP Layer= 4.882835439599452 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 172 J= 2.136406488272867 Error Rate= 0.728\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.815393822403052 \n",
      "Max abs(dJda) of Last MLP Layer= 1.787520052294076 \n",
      "Max abs(W) of Last MLP Layer= 4.882779211404393 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 173 J= 2.0464659941677636 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.895904600522799 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4035742802109603 \n",
      "Max abs(W) of Last MLP Layer= 4.882725071181776 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 174 J= 2.0982242862154465 Error Rate= 0.656\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.703852597212342 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3386332839649728 \n",
      "Max abs(W) of Last MLP Layer= 4.882674332907644 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 175 J= 2.088419652279248 Error Rate= 0.664\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.285447283986625 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3462698843881875 \n",
      "Max abs(W) of Last MLP Layer= 4.882614015264246 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 176 J= 2.152018133462085 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.307055066428747 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2214346994692111 \n",
      "Max abs(W) of Last MLP Layer= 4.882556538678704 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 177 J= 2.1035392163445037 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.054162351463117 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4592048447558184 \n",
      "Max abs(W) of Last MLP Layer= 4.882495546398557 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 178 J= 2.0282870029162794 Error Rate= 0.64\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.49361278505089 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3798118807453017 \n",
      "Max abs(W) of Last MLP Layer= 4.882437299703928 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 179 J= 2.079557994570385 Error Rate= 0.664\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.815100386675978 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5824397975174158 \n",
      "Max abs(W) of Last MLP Layer= 4.882377463861337 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 180 J= 2.1678446963918887 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.807602685331051 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2558237302946427 \n",
      "Max abs(W) of Last MLP Layer= 4.882317485294753 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 181 J= 2.1247850623650057 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.393950113049329 \n",
      "Max abs(dJda) of Last MLP Layer= 1.366924953473815 \n",
      "Max abs(W) of Last MLP Layer= 4.882259813197115 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 182 J= 1.9767549634372354 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.875999811108322 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2999260324889423 \n",
      "Max abs(W) of Last MLP Layer= 4.8822008399899 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 183 J= 2.011085606823869 Error Rate= 0.656\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.291004519728656 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5515879534316628 \n",
      "Max abs(W) of Last MLP Layer= 4.882132232845842 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 184 J= 1.9764817199743603 Error Rate= 0.624\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.042907807869884 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2036562056904359 \n",
      "Max abs(W) of Last MLP Layer= 4.882082668886063 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 185 J= 2.1127490230140644 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.179609910767613 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5852362180183128 \n",
      "Max abs(W) of Last MLP Layer= 4.882030108957837 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 186 J= 2.1692027747956217 Error Rate= 0.72\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.77902017720156 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3544671141743743 \n",
      "Max abs(W) of Last MLP Layer= 4.881961467205444 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 187 J= 2.0831919151984466 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.040836743149195 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3255947028611343 \n",
      "Max abs(W) of Last MLP Layer= 4.881890844326287 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 188 J= 2.047969827146625 Error Rate= 0.652\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.9265394126353526 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5178338257797634 \n",
      "Max abs(W) of Last MLP Layer= 4.881832217272295 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 189 J= 2.1332130358445545 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.351940947697834 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3184548871869748 \n",
      "Max abs(W) of Last MLP Layer= 4.881766405388051 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 190 J= 2.0592636350246063 Error Rate= 0.664\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.0671382247858885 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2769667826892026 \n",
      "Max abs(W) of Last MLP Layer= 4.8817122567302595 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 191 J= 2.124371867568022 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.805097871240344 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3815014383593605 \n",
      "Max abs(W) of Last MLP Layer= 4.881648091057283 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 192 J= 2.0590002649716226 Error Rate= 0.656\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.0334252824784995 \n",
      "Max abs(dJda) of Last MLP Layer= 1.30323810922276 \n",
      "Max abs(W) of Last MLP Layer= 4.881594764437602 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 193 J= 1.917018576409967 Error Rate= 0.58\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.075198966972589 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3801386901602486 \n",
      "Max abs(W) of Last MLP Layer= 4.881540578126586 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 194 J= 2.110116204457938 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.7774262290097465 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4325474156983222 \n",
      "Max abs(W) of Last MLP Layer= 4.881487696081747 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 195 J= 1.9911015482262922 Error Rate= 0.648\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.354467244293118 \n",
      "Max abs(dJda) of Last MLP Layer= 1.1128044710659 \n",
      "Max abs(W) of Last MLP Layer= 4.881431972305577 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Batch 196 J= 2.091182987865965 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.258114321412341 \n",
      "Max abs(dJda) of Last MLP Layer= 1.8182821408742882 \n",
      "Max abs(W) of Last MLP Layer= 4.88136195100478 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 197 J= 2.1597682274064978 Error Rate= 0.724\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.188150200513139 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7285297007687732 \n",
      "Max abs(W) of Last MLP Layer= 4.881298256726095 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 198 J= 2.105679269159176 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.485381801021719 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2068686608780923 \n",
      "Max abs(W) of Last MLP Layer= 4.881238339183915 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 199 J= 2.10952126607697 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.2736248927048734 \n",
      "Max abs(dJda) of Last MLP Layer= 1.235411762903548 \n",
      "Max abs(W) of Last MLP Layer= 4.881178229906412 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 200 J= 2.193575297112867 Error Rate= 0.716\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.331199190728458 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4918761588925773 \n",
      "Max abs(W) of Last MLP Layer= 4.881117439255621 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 201 J= 2.10869497309488 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.7052727168222965 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6183337882608069 \n",
      "Max abs(W) of Last MLP Layer= 4.88106172339052 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 202 J= 2.231508984318133 Error Rate= 0.752\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.512882638238313 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6806294178658887 \n",
      "Max abs(W) of Last MLP Layer= 4.881012575924451 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 203 J= 2.112200231092136 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.850847497451156 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2763657268856512 \n",
      "Max abs(W) of Last MLP Layer= 4.880934195874338 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 204 J= 2.184727150090776 Error Rate= 0.732\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.66700147329165 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5804982509887282 \n",
      "Max abs(W) of Last MLP Layer= 4.880865967484352 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 205 J= 2.0214802791670614 Error Rate= 0.62\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.3623899168372615 \n",
      "Max abs(dJda) of Last MLP Layer= 1.388442983024343 \n",
      "Max abs(W) of Last MLP Layer= 4.880800735502894 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 206 J= 2.064980858489726 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.90522249074308 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4550787404981054 \n",
      "Max abs(W) of Last MLP Layer= 4.880734731262672 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 207 J= 2.0717586206579304 Error Rate= 0.668\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.923804027876012 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4724373613226382 \n",
      "Max abs(W) of Last MLP Layer= 4.880681236388771 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 208 J= 2.2163402024612537 Error Rate= 0.716\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.837408480436283 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5704070450890146 \n",
      "Max abs(W) of Last MLP Layer= 4.880619308578383 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 209 J= 2.0371041412355977 Error Rate= 0.648\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.618562256310045 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4135001107102751 \n",
      "Max abs(W) of Last MLP Layer= 4.880557298125926 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 210 J= 2.0707006200452067 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.105053763398307 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3893421442812666 \n",
      "Max abs(W) of Last MLP Layer= 4.880502275893438 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 211 J= 2.047428104206091 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.795553308401241 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2707376780979203 \n",
      "Max abs(W) of Last MLP Layer= 4.8804371855403055 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 212 J= 2.064693126903329 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.0067323393529755 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4194444873814396 \n",
      "Max abs(W) of Last MLP Layer= 4.880383219340874 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 213 J= 2.0335825756617827 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.720697295291984 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5148337899933724 \n",
      "Max abs(W) of Last MLP Layer= 4.880322904186826 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 214 J= 2.140542293271925 Error Rate= 0.7\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.500980864606439 \n",
      "Max abs(dJda) of Last MLP Layer= 1.478603560790663 \n",
      "Max abs(W) of Last MLP Layer= 4.880269544967793 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 215 J= 1.980388476352788 Error Rate= 0.608\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.436053639207726 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3940806892607116 \n",
      "Max abs(W) of Last MLP Layer= 4.880215682908218 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 216 J= 2.139985953990338 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.072188470073204 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2998285872226676 \n",
      "Max abs(W) of Last MLP Layer= 4.880151341728343 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 217 J= 2.114205416154264 Error Rate= 0.684\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.227584723259533 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4011031716160667 \n",
      "Max abs(W) of Last MLP Layer= 4.880093708926085 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 218 J= 2.074914774097698 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.5742652413592895 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5076840695758165 \n",
      "Max abs(W) of Last MLP Layer= 4.880030283595278 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 219 J= 2.0851711783792295 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.703448200304706 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5690187842872594 \n",
      "Max abs(W) of Last MLP Layer= 4.87996836632921 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 220 J= 2.0049400001949325 Error Rate= 0.656\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.535458265673262 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2568826451075392 \n",
      "Max abs(W) of Last MLP Layer= 4.879911724147261 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 221 J= 2.111123207099191 Error Rate= 0.696\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.612900284884356 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2899354250138815 \n",
      "Max abs(W) of Last MLP Layer= 4.8798493209811 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 222 J= 2.0257761058053054 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.072652488047011 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6091186903334016 \n",
      "Max abs(W) of Last MLP Layer= 4.879788553742836 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 223 J= 1.9827417256537183 Error Rate= 0.624\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 4.647815938880275 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2893426726239101 \n",
      "Max abs(W) of Last MLP Layer= 4.879746618290931 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 224 J= 2.075768978387467 Error Rate= 0.664\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.68182448850962 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6769699372881761 \n",
      "Max abs(W) of Last MLP Layer= 4.879682441119426 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 225 J= 1.9948448283938354 Error Rate= 0.616\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.879138745507263 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2830239565819392 \n",
      "Max abs(W) of Last MLP Layer= 4.879618965183294 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 226 J= 2.00575672027945 Error Rate= 0.648\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.522789487896577 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4899361427129794 \n",
      "Max abs(W) of Last MLP Layer= 4.8795642979071125 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 227 J= 2.0812209341911263 Error Rate= 0.672\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.290064542564091 \n",
      "Max abs(dJda) of Last MLP Layer= 1.09099270319608 \n",
      "Max abs(W) of Last MLP Layer= 4.879515381876406 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 228 J= 2.0295512375175973 Error Rate= 0.66\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.612226525987155 \n",
      "Max abs(dJda) of Last MLP Layer= 1.6771675658174499 \n",
      "Max abs(W) of Last MLP Layer= 4.879455923389108 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 229 J= 2.0156257053578965 Error Rate= 0.668\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.6682651478655846 \n",
      "Max abs(dJda) of Last MLP Layer= 1.403958577875618 \n",
      "Max abs(W) of Last MLP Layer= 4.879400134792061 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 230 J= 2.106364311454054 Error Rate= 0.68\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.048311784279404 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4279404924205172 \n",
      "Max abs(W) of Last MLP Layer= 4.879338709919674 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 231 J= 2.1494373715114157 Error Rate= 0.712\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.89868006057017 \n",
      "Max abs(dJda) of Last MLP Layer= 1.3848358165536536 \n",
      "Max abs(W) of Last MLP Layer= 4.879274274864718 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 232 J= 1.9921032990888392 Error Rate= 0.644\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.338525226483717 \n",
      "Max abs(dJda) of Last MLP Layer= 1.076098743552613 \n",
      "Max abs(W) of Last MLP Layer= 4.879209839235537 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 233 J= 2.0287040248349486 Error Rate= 0.616\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.267285549417443 \n",
      "Max abs(dJda) of Last MLP Layer= 1.7457758258770144 \n",
      "Max abs(W) of Last MLP Layer= 4.879145536152293 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 234 J= 2.173540387184329 Error Rate= 0.704\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.940883943633221 \n",
      "Max abs(dJda) of Last MLP Layer= 1.4731304679659232 \n",
      "Max abs(W) of Last MLP Layer= 4.879080470719657 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Batch 235 J= 2.0143210021057816 Error Rate= 0.676\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.9480449020106665 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2684976780257982 \n",
      "Max abs(W) of Last MLP Layer= 4.879025523184017 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 236 J= 2.005959435737979 Error Rate= 0.632\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.0876190150551635 \n",
      "Max abs(dJda) of Last MLP Layer= 1.5297461177445228 \n",
      "Max abs(W) of Last MLP Layer= 4.878962659203254 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 237 J= 2.080485762506192 Error Rate= 0.692\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 6.745360294179294 \n",
      "Max abs(dJda) of Last MLP Layer= 1.389909940838945 \n",
      "Max abs(W) of Last MLP Layer= 4.87890002004301 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 238 J= 2.0784134947112745 Error Rate= 0.708\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.856341632922945 \n",
      "Max abs(dJda) of Last MLP Layer= 1.0918100627834826 \n",
      "Max abs(W) of Last MLP Layer= 4.878838360152667 \n",
      "\n",
      "\n",
      "Epoch 0 Batch 239 J= 2.041884449971188 Error Rate= 0.668\n",
      "\n",
      "Max abs(a) of Last MLP Layer= 5.363665053737062 \n",
      "Max abs(dJda) of Last MLP Layer= 1.2288518559419441 \n",
      "Max abs(W) of Last MLP Layer= 4.878782209164279 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a2a7e1f66fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn60000\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_60000_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_60000_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_60000_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_60000_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpochCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLearningRate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run time for is    \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"   seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-87f0c364bc82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_x, train_y, val_x, val_y, epoch_count, lr, eta)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-87f0c364bc82>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Flatten the last output feature map of the conv layer for feeding to the MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-44da8c05f72b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ai --> z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# z --> bnz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# bnz --> r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-44da8c05f72b>\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_ch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# z_ch == k_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                         self.z[b, c, i, j] = np.sum(np.multiply(self.W[c, :,  :, :],   #multipy two matrix\n\u001b[1;32m     70\u001b[0m                                                                 self.ai[b,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "cnn60000.train(train_60000_x, train_60000_y,train_60000_x, train_60000_y, EpochCount, LearningRate, Eta)\n",
    "print(\"Run time for is    \",time.clock() - start_time, \"   seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN result : error = ~0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
